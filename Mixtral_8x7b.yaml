model:
  name: "Mixtral 8x7b"
  global_batch_size: 1024
  micro_batch_size: 1
  vocab_size: 32000
  num_layers: 32
  hidden_size: 4096
  ffn_hidden_size: 14336
  seq_length: 32768
  activation: "swiglu"
  moe:
    expert_frequency: 1.
    k: 2
    num_experts: 8
    token_imbalance_hypothesis: 1.0
trainer:
  data_parallel_sharding_strategy: "OPTIMIZER_STATES"
  param_dtype: "float16"
  grad_dtype: "float32"
