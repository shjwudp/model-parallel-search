model:
  name: "LLaMA-3.1 70B"
  global_batch_size: 4096
  micro_batch_size: 1
  vocab_size: 128256
  num_layers: 80
  hidden_size: 8192
  num_attention_heads: 64
  num_key_value_heads: 8
  ffn_hidden_size: 28672
  seq_length: 8192
  activation: "swiglu"
  share_embeddings_and_output_weights: False
trainer:
  data_parallel_sharding_strategy: "OPTIMIZER_STATES"
  param_dtype: "float16"
  grad_dtype: "float32"
